{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.3\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NYCTaxiAnalysis\").getOrCreate()\n",
    "df_taxi = spark.read.option(\"header\", \"true\").csv(\"data/Sample NYC Data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+---------+---------+------------------+---------------+----------------+---------------+----------------+---------------+-----------------+----------------+\n",
      "|           medallion|        hack_license|vendor_id|rate_code|store_and_fwd_flag|pickup_datetime|dropoff_datetime|passenger_count|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|\n",
      "+--------------------+--------------------+---------+---------+------------------+---------------+----------------+---------------+----------------+---------------+-----------------+----------------+\n",
      "|89D227B655E5C82AE...|BA96DE419E711691B...|      CMT|        1|                 N| 01-01-13 15:11|  01-01-13 15:18|              4|      -73.978165|      40.757977|       -73.989838|       40.751171|\n",
      "|0BD7C8F5BA12B88E0...|9FD8F69F0804BDB55...|      CMT|        1|                 N| 06-01-13 00:18|  06-01-13 00:22|              1|      -74.006683|      40.731781|       -73.994499|        40.75066|\n",
      "|0BD7C8F5BA12B88E0...|9FD8F69F0804BDB55...|      CMT|        1|                 N| 05-01-13 18:49|  05-01-13 18:54|              1|      -74.004707|       40.73777|       -74.009834|       40.726002|\n",
      "|DFD2202EE08F7A8DC...|51EE87E3205C985EF...|      CMT|        1|                 N| 07-01-13 23:54|  07-01-13 23:58|              2|      -73.974602|      40.759945|       -73.984734|       40.759388|\n",
      "|DFD2202EE08F7A8DC...|51EE87E3205C985EF...|      CMT|        1|                 N| 07-01-13 23:25|  07-01-13 23:34|              1|       -73.97625|      40.748528|       -74.002586|       40.747868|\n",
      "+--------------------+--------------------+---------+---------+------------------+---------------+----------------+---------------+----------------+---------------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi.printSchema()\n",
    "df_taxi.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shapely\n",
      "  Downloading shapely-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.11/site-packages (from shapely) (1.26.4)\n",
      "Downloading shapely-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: shapely\n",
      "Successfully installed shapely-2.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from shapely.geometry import shape\n",
    "\n",
    "with open(\"data/nyc-boroughs.geojson\", \"r\") as f:\n",
    "    geojson_data = json.load(f)\n",
    "\n",
    "# Extract features\n",
    "features = geojson_data[\"features\"]\n",
    "\n",
    "# Sort features by borough code and area (descending)\n",
    "def feature_sort_key(f):\n",
    "    borough_code = f[\"properties\"][\"boroughCode\"]\n",
    "    polygon_area = shape(f[\"geometry\"]).area\n",
    "    return (borough_code, polygon_area * -1)  # negative for descending on area\n",
    "\n",
    "features_sorted = sorted(features, key=feature_sort_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast the GeoJSON\n",
    "bc_features = spark.sparkContext.broadcast(features_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, col\n",
    "\n",
    "df_taxi = df_taxi.withColumn(\n",
    "    \"pickup_ts\",\n",
    "    unix_timestamp(col(\"pickup_datetime\"), \"dd-MM-yy HH:mm\")\n",
    ")\n",
    "\n",
    "df_taxi = df_taxi.withColumn(\n",
    "    \"dropoff_ts\",\n",
    "    unix_timestamp(col(\"dropoff_datetime\"), \"dd-MM-yy HH:mm\")\n",
    ")\n",
    "\n",
    "df_taxi = df_taxi.withColumn(\n",
    "    \"duration_sec\",\n",
    "    col(\"dropoff_ts\") - col(\"pickup_ts\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      duration_sec|\n",
      "+-------+------------------+\n",
      "|  count|             99999|\n",
      "|    min|                 0|\n",
      "|    max|              9180|\n",
      "|   mean| 650.6807068070681|\n",
      "| stddev|469.78651920883175|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi.select(\"duration_sec\").summary(\"count\", \"min\", \"max\", \"mean\", \"stddev\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Remove negative durations, zero durations, and those above 10800 seconds (3 hours).\n",
    "df_taxi = df_taxi.filter((col(\"duration_sec\") > 0) & (col(\"duration_sec\") <= 10800))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to Map Coordinates → Borough\n",
    "from shapely.geometry import Point\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def get_borough(lon, lat, features):\n",
    "    if lon is None or lat is None:\n",
    "        return None\n",
    "    point = Point(float(lon), float(lat))\n",
    "    for f in features:\n",
    "        polygon = shape(f[\"geometry\"])\n",
    "        if polygon.contains(point):\n",
    "            return f[\"properties\"][\"borough\"]\n",
    "    return None\n",
    "\n",
    "def udf_get_borough(lon, lat):\n",
    "    return get_borough(lon, lat, bc_features.value)\n",
    "\n",
    "borough_udf = udf(udf_get_borough)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_taxi = df_taxi.withColumn(\"pickup_borough\",\n",
    "    borough_udf(col(\"pickup_longitude\"), col(\"pickup_latitude\"))\n",
    ")\n",
    "\n",
    "df_taxi = df_taxi.withColumn(\"dropoff_borough\",\n",
    "    borough_udf(col(\"dropoff_longitude\"), col(\"dropoff_latitude\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99549"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caching the DataFrame\n",
    "df_taxi.cache()\n",
    "df_taxi.count()  # trigger the cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partion and sort\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "window_spec = Window.partitionBy(\"medallion\").orderBy(\"pickup_ts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Idle Time\n",
    "# Compare the pickup time of the current trip to the dropoff time of the previous trip:\n",
    "df_taxi = df_taxi.withColumn(\n",
    "  \"prev_dropoff_ts\",\n",
    "  F.lag(\"dropoff_ts\").over(window_spec)\n",
    ")\n",
    "df_taxi = df_taxi.withColumn(\n",
    "  \"idle_time_sec\",\n",
    "  col(\"pickup_ts\") - col(\"prev_dropoff_ts\")\n",
    ")\n",
    "# Replace null with 0 for the first trip\n",
    "df_taxi = df_taxi.fillna({\"idle_time_sec\": 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore very large idle times (e.g., >4 hours) as new sessions. Maybe we do not need this.\n",
    "df_taxi = df_taxi.withColumn(\n",
    "  \"idle_time_sec\",\n",
    "  F.when(col(\"idle_time_sec\") < 14400, col(\"idle_time_sec\")).otherwise(0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Required Queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilization (per Taxi/Driver)\n",
    "# “Fraction of time a taxi is occupied.”\n",
    "#Summation of driving time vs. total time (driving + idle)\n",
    "\n",
    "df_trip_time = df_taxi.groupBy(\"medallion\") \\\n",
    "    .agg(F.sum(\"duration_sec\").alias(\"sum_trip_time\"))\n",
    "\n",
    "df_idle_time = df_taxi.groupBy(\"medallion\") \\\n",
    "    .agg(F.sum(\"idle_time_sec\").alias(\"sum_idle_time\"))\n",
    "\n",
    "df_util = df_trip_time.join(df_idle_time, on=\"medallion\") \\\n",
    "    .withColumn(\n",
    "        \"utilization\",\n",
    "        F.col(\"sum_trip_time\") / (F.col(\"sum_trip_time\") + F.col(\"sum_idle_time\"))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|dropoff_borough|      avg_idle_sec|\n",
      "+---------------+------------------+\n",
      "|         Queens|1186.9237217099749|\n",
      "|           NULL|1124.9278460716193|\n",
      "|       Brooklyn|1163.2585433206766|\n",
      "|  Staten Island|            2736.0|\n",
      "|      Manhattan|1011.4870562796565|\n",
      "|          Bronx| 1261.851851851852|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average Time to Next Fare (per Destination Borough)\n",
    "\n",
    "# \"lead\" so we can shift the idle time back one row\n",
    "df_taxi = df_taxi.withColumn(\n",
    "  \"idle_time_for_this_dropoff\",\n",
    "  F.lag(\"idle_time_sec\").over(window_spec)\n",
    ")\n",
    "# group by dropoff_borough\n",
    "df_avg_idle = df_taxi.groupBy(\"dropoff_borough\") \\\n",
    "    .agg(F.avg(\"idle_time_for_this_dropoff\").alias(\"avg_idle_sec\"))\n",
    "df_avg_idle.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|pickup_borough|count|\n",
      "+--------------+-----+\n",
      "|        Queens| 1369|\n",
      "|      Brooklyn| 1062|\n",
      "| Staten Island|    1|\n",
      "|     Manhattan|83463|\n",
      "|         Bronx|   49|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of Trips that Start and End in the Same Borough\n",
    "\n",
    "df_same = df_taxi.filter(col(\"pickup_borough\") == col(\"dropoff_borough\"))\n",
    "count_same = df_same.count()\n",
    "\n",
    "# broken down by borough\n",
    "df_same.groupBy(\"pickup_borough\").count().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-----+\n",
      "|pickup_borough|dropoff_borough|count|\n",
      "+--------------+---------------+-----+\n",
      "|      Brooklyn|      Manhattan|  773|\n",
      "|        Queens|          Bronx|  100|\n",
      "|      Brooklyn|         Queens|  115|\n",
      "|        Queens|  Staten Island|    2|\n",
      "|     Manhattan|  Staten Island|    9|\n",
      "|     Manhattan|       Brooklyn| 1923|\n",
      "|     Manhattan|         Queens| 3943|\n",
      "|     Manhattan|          Bronx|  244|\n",
      "|        Queens|      Manhattan| 3697|\n",
      "|         Bronx|      Manhattan|   25|\n",
      "|        Queens|       Brooklyn|  597|\n",
      "|         Bronx|         Queens|    2|\n",
      "| Staten Island|         Queens|    1|\n",
      "+--------------+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of Trips from One Borough to Another\n",
    "\n",
    "df_diff = df_taxi.filter(col(\"pickup_borough\") != col(\"dropoff_borough\"))\n",
    "count_diff = df_diff.count()\n",
    "\n",
    "# Per borough pair\n",
    "df_diff.groupBy(\"pickup_borough\", \"dropoff_borough\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util.coalesce(1).write.csv(\"save\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

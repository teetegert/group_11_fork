{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.3\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 0 - Data Cleansing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "# Create schema for csv data\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"surcharge\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "|5EE2C4D3BF57BDB45...|E96EF8F6E6122591F...|2013-01-01 00:00:09|2013-01-01 00:00:36|               26|          0.1|       -73.99221|      40.725124|       -73.991646|       40.726658|         CSH|        2.5|      0.5|    0.5|       0.0|         0.0|\n",
      "|42730E78D8BE872B5...|6016A71F1D29D678E...|2013-01-01 00:01:00|2013-01-01 00:01:00|                0|         0.01|             0.0|            0.0|              0.0|             0.0|         CSH|        2.5|      0.5|    0.5|       0.0|         0.0|\n",
      "|CA6CD9BAED6A85E43...|77FFDF38272A60065...|2013-01-01 00:00:20|2013-01-01 00:01:22|               61|          2.2|        -73.9701|      40.768005|       -73.969772|       40.767834|         CSH|        3.0|      0.5|    0.5|       0.0|         0.0|\n",
      "|15162141EA7436635...|CDCB7729DE0724372...|2013-01-01 00:00:14|2013-01-01 00:01:37|               83|          0.2|      -73.975441|      40.749657|       -73.977333|       40.751991|         CSH|        3.0|      0.5|    0.5|       0.0|         0.0|\n",
      "|025B98A22ED771118...|7D89374F8E98F30A1...|2013-01-01 00:00:40|2013-01-01 00:01:40|               60|          0.3|      -74.005165|      40.720531|       -74.003929|       40.725655|         CSH|        3.0|      0.5|    0.5|       0.0|         0.0|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV data\n",
    "df = spark.read.schema(schema).csv(\"input/sorted_data.csv\")\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial row count: 173185091\n"
     ]
    }
   ],
   "source": [
    "initial_count = df.count()\n",
    "print(f\"Initial row count: {initial_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data, drop null values that might have been added by giving the data a schema.\n",
    "df_clean = df.dropna()\n",
    "# Filter out wrong values\n",
    "df_clean = df_clean.filter(df_clean.trip_time_in_secs > 0)\n",
    "df_clean = df_clean.filter(df_clean.trip_distance > 0)\n",
    "df_clean = df_clean.filter(df_clean.hack_license != \"unknown\")\n",
    "df_clean = df_clean.filter(df_clean.medallion != \"unknown\")\n",
    "\n",
    "df_clean = df_clean.filter(df_clean.pickup_latitude != 0.0)\n",
    "df_clean = df_clean.filter(df_clean.pickup_longitude != 0.0)\n",
    "df_clean = df_clean.filter(df_clean.dropoff_latitude != 0.0)\n",
    "df_clean = df_clean.filter(df_clean.dropoff_longitude != 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "#Create new column for partitioning\n",
    "df_clean = df_clean.withColumn(\"trip_date\", to_date(df_clean.pickup_datetime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count after cleaning: 169346433\n"
     ]
    }
   ],
   "source": [
    "# Count total rows after cleaning\n",
    "final_count = df_clean.count()\n",
    "print(f\"Row count after cleaning: {final_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|max(pickup_datetime)|\n",
      "+--------------------+\n",
      "| 2013-12-31 23:59:57|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count total rows after cleaning\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "df_clean.head()\n",
    "df_clean.select(max(\"pickup_datetime\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|max(pickup_datetime)|\n",
      "+--------------------+\n",
      "| 2013-12-31 23:59:57|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count total rows after cleaning\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "df.head()\n",
    "df.select(max(\"pickup_datetime\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only a sample of the cleaned data due to memory issues\n",
    "df_sample = df_clean.sample(fraction=0.1, seed=42)\n",
    "# Create daily partitions to output folder\n",
    "df_sample.write.partitionBy(\"trip_date\").mode(\"overwrite\").parquet(\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka osa on pooleli, hetkel üritasin kasutada partitioneid mis lõin outputi, et need üks haaval kafkasse sisse lugeda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "partition_dirs = [\n",
    "    os.path.join(\"output\", d)\n",
    "    for d in os.listdir(\"output\")\n",
    "    if d.startswith(\"trip_date=\")\n",
    "]\n",
    "for partition_path in partition_dirs:\n",
    "    print(f\"Writing data from: {partition_path}\")\n",
    "    df_partition = spark.read.parquet(partition_path)\n",
    "    df_partition.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the data to Kafka for future use.\n",
    "df_sample.selectExpr(\"CAST(medallion AS STRING) AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"cleaned_data\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    current_timestamp, make_timestamp, month, dayofmonth, hour, minute, second, lit,\n",
    "    expr, col, floor, concat_ws, count\n",
    ")\n",
    "\n",
    "def get_adjusted_window() -> tuple:\n",
    "    now = current_timestamp()\n",
    "    adjusted_now = make_timestamp(\n",
    "        lit(2013),\n",
    "        month(now),\n",
    "        dayofmonth(now),\n",
    "        hour(now),\n",
    "        minute(now),\n",
    "        second(now)\n",
    "    )\n",
    "    window_start = expr(\"\"\"\n",
    "        make_timestamp(2013,\n",
    "                       month(current_timestamp()),\n",
    "                       day(current_timestamp()),\n",
    "                       hour(current_timestamp()),\n",
    "                       minute(current_timestamp()),\n",
    "                       second(current_timestamp()))\n",
    "        - interval 30 minutes\n",
    "    \"\"\")\n",
    "    return adjusted_now, window_start\n",
    "\n",
    "\n",
    "def get_window_dates(adjusted_now, window_start) -> tuple:\n",
    "    adjusted_date_str = adjusted_now.cast(\"date\")\n",
    "    window_date_str = window_start.cast(\"date\")\n",
    "    return adjusted_date_str, window_date_str\n",
    "\n",
    "\n",
    "def load_filtered_parquet(base_path: str, adjusted_date, window_date) -> DataFrame:\n",
    "    return spark.read.option(\"basePath\", base_path).parquet(base_path) \\\n",
    "        .filter((col(\"trip_date\") == adjusted_date) | (col(\"trip_date\") == window_date))\n",
    "\n",
    "\n",
    "def add_grid_cells(df: DataFrame, grid_size: float = 0.01) -> DataFrame:\n",
    "    return df \\\n",
    "        .withColumn(\"start_cell_lat\", floor(col(\"pickup_latitude\") / grid_size)) \\\n",
    "        .withColumn(\"start_cell_lon\", floor(col(\"pickup_longitude\") / grid_size)) \\\n",
    "        .withColumn(\"start_cell\", concat_ws(\"_\", col(\"start_cell_lat\"), col(\"start_cell_lon\"))) \\\n",
    "        .withColumn(\"end_cell_lat\", floor(col(\"dropoff_latitude\") / grid_size)) \\\n",
    "        .withColumn(\"end_cell_lon\", floor(col(\"dropoff_longitude\") / grid_size)) \\\n",
    "        .withColumn(\"end_cell\", concat_ws(\"_\", col(\"end_cell_lat\"), col(\"end_cell_lon\")))\n",
    "\n",
    "\n",
    "def get_top_routes(df: DataFrame, adjusted_now, window_start, top_n: int = 10) -> DataFrame:\n",
    "    filtered = df.filter(\n",
    "        (col(\"dropoff_datetime\") > window_start) &\n",
    "        (col(\"dropoff_datetime\") <= adjusted_now)\n",
    "    )\n",
    "    return filtered.groupBy(\"start_cell\", \"end_cell\") \\\n",
    "        .agg(count(\"*\").alias(\"Number of Rides\")) \\\n",
    "        .orderBy(col(\"Number of Rides\").desc()) \\\n",
    "        .limit(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------------+\n",
      "|start_cell|  end_cell|Number of Rides|\n",
      "+----------+----------+---------------+\n",
      "|4075_-7398|4074_-7399|             10|\n",
      "|4075_-7399|4075_-7398|             10|\n",
      "|4075_-7398|4075_-7399|              9|\n",
      "|4074_-7399|4075_-7399|              8|\n",
      "|4074_-7399|4075_-7398|              8|\n",
      "|4076_-7398|4075_-7398|              8|\n",
      "|4077_-7397|4076_-7397|              7|\n",
      "|4074_-7400|4075_-7399|              7|\n",
      "|4077_-7396|4077_-7395|              6|\n",
      "|4075_-7400|4075_-7398|              6|\n",
      "+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adjusted_now, window_start = get_adjusted_window()\n",
    "adjusted_date, window_date = get_window_dates(adjusted_now, window_start)\n",
    "\n",
    "df = load_filtered_parquet(\"output/\", adjusted_date, window_date)\n",
    "df_with_cells = add_grid_cells(df)\n",
    "\n",
    "top_routes = get_top_routes(df_with_cells, adjusted_now, window_start)\n",
    "top_routes.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
